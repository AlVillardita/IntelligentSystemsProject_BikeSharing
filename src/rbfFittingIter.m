function [ mse ] = rbfFittingIter( dataSet, features, samplesIntervals, targets, goal, spreadValues, iterations, labels )
% rbfFittingIter.m: the same as rbfFitting.m but here repeating the
% training as many times as specified by the parameter 'iterations', so to
% generate #'iterations' samples.
%
% Data set division: here the given data set is divided into two subsets:
% the training and the testing sets. More in detail, the two are generated
% starting from a set of partitions on the data set. That is: the training
% set is generated by sampling 70% of elements for each partition defined
% in 'samplesIntervals'. It has been used to perform a monthly based
% sampling, so to achieve lower MSEs.

% @Authors: Alessio Villardita [villardita.alessio@gmail.com]
%           Sara Egidi [egidi.sara@gmail.com]


train_ratio = 70/100;
MAX_NEURONS = 60;

fprintf('Features selected = ');
for f = features
    fprintf('%s ', char(labels(f)));
end
fprintf('\n');

%Setting sets dimensions
% Compared with dividerand, the resulting data sets differ only when
% working with the hour data set: in this case, the training set has 2 more
% samples (12167) than the one resulting from dividerand (12165).

data_set_size = numel(dataSet(:,1));
training_set_size = round(data_set_size * train_ratio);

training_set_idx = zeros(training_set_size,1);

numel_samples_intervals = numel(samplesIntervals(:,1));

base_idx = 1;
%building the training set
for i = 1:numel_samples_intervals
    sample_size = samplesIntervals(i,2)-samplesIntervals(i,1)+1;
    training_sample_size = round(sample_size * train_ratio);
    sample_idx_interval = (samplesIntervals(i,1):samplesIntervals(i,2))';
    training_sample_idx = datasample(sample_idx_interval, training_sample_size, 1, 'Replace', false);
    training_set_idx(base_idx:base_idx+training_sample_size-1,1) = training_sample_idx;
    base_idx = base_idx + training_sample_size;
end

idx = (1:data_set_size)';
test_set_idx = setdiff(idx, training_set_idx);

% Creating training and testing sets
training_set = dataSet(training_set_idx,features);
test_set = dataSet(test_set_idx,features);

fprintf('Training set size = %d \t Test set size = %d\n', numel(training_set_idx), numel(test_set_idx));


% Computing MSE for each spread value
num_of_spreads = numel(spreadValues);
i = 1;
mse = zeros(num_of_spreads,1);
for s = spreadValues
    fprintf('%0.2f progress - Spread = %0.2f\n', (i*100/num_of_spreads),s);
    for j = 1:iterations
        net = newrb(training_set',targets(training_set_idx)', goal, s, MAX_NEURONS);
        outputs = sim(net,test_set');
        mse(i,1) = mse(i,1) + perform(net, targets(test_set_idx)', outputs);
    end
    mse(i,1) =  mse(i,1)/iterations;
    i = i + 1;
end
plot(spreadValues,mse);
end
